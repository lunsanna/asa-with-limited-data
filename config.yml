# This config file contains all the params used for training
# Some of them are taken from Yaroslav's shell script

# TODO: revise and remove unused ones

model_args:
  # The name or path of the pre-trained model to use as a starting point
  # Swedish
  sv_pretrained: "KBLab/wav2vec2-large-voxrex-swedish"
  # Finnish, not publicly available
  fi_pretrained: "/scratch/elec/puhe/p/getmany1/wav2vec2_large_14.2k_fi_donatespeech_100h_SEGMENTED_13042022_60ep/checkpoint-11100"

  # Directory to store output files, including model checkpoints
  output_dir: "output"

  # Directory to store cached files for faster data loading
  cache_dir: "cache"

  # Whether to freeze the feature extractor
  freeze_feature_encoder: true

  # Whether to enable verbose logging
  verbose_logging: true

data_args:
  # The path of the dataset to use, not on git 
  csv_fi: "finnish_df.csv"
  csv_sv: "swedish_df.csv"

  # The name of the training split
  train_split_name: "train"

  # The name of the validation split, "train" for training error, "validation" for test error
  validation_split_name: "train"

  # The orthography used for normalization and tokenization: 'librispeech' (default), 'timit', or 'buckwalter'.
  orthography: "timit"

  # Target sampling rate for the feature extractor, replace with a specific value
  target_feature_extractor_sampling_rate: 16000
  
  # Number of workers for data preprocessing, replace "$(nproc)" with a specific number
  # TODO: Handle this in the python code.
  # preprocessing_num_workers: "$(nproc)"


training_args:
  # Number of training epochs
  num_train_epochs: 20

  # Batch size for training
  per_device_train_batch_size: 1

  # Batch size for evaluation
  per_device_eval_batch_size: 1

  # Number of steps to accumulate gradients before performing an update
  gradient_accumulation_steps: 4

  # The maximum number of model checkpoint files to keep
  save_total_limit: 10

  # The strategy for evaluation during training, "steps" means evaluating at each logging step
  evaluation_strategy: "epoch"
  # eval_steps: 1

  # The strategy for logging during training, "steps" means logging at each training step
  logging_strategy: "epoch"

  # The strategy for saving the model, "steps" means saving at each logging step
  save_strategy: "epoch"
  # save_steps: 1

  learning_rate: 0.0001

  warmup_ratio: 0.1

  # Whether to group samples of similar length together
  group_by_length: true

  # Whether to enable gradient checkpointing for memory efficiency
  gradient_checkpointing: false

  # Whether to enable mixed precision training, only availble with CUDA
  fp16: true

  # The backend to use for half precision, "cuda_amp" for NVIDIA's Automatic Mixed Precision
  half_precision_backend: "cuda_amp"

  # Whether to load the best model found during training at the end of training
  load_best_model_at_end: true

  # The metric to use to determine the best model
  metric_for_best_model: "wer"

  # Whether a higher metric score means a better model, false for Word Error Rate (WER) because a lower WER is better
  greater_is_better: false

  ddp_find_unused_parameters: true



























