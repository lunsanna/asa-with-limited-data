Map:   0%|          | 0/15 [00:00<?, ? examples/s]Map: 100%|██████████| 15/15 [00:00<00:00, 132.13 examples/s]                                                            Map:   0%|          | 0/5 [00:00<?, ? examples/s]                                                 Map (num_proc=6):   0%|          | 0/15 [00:00<?, ? examples/s]Map (num_proc=6):   7%|▋         | 1/15 [00:00<00:04,  2.84 examples/s]Map (num_proc=6):  27%|██▋       | 4/15 [00:00<00:01,  7.91 examples/s]Map (num_proc=6):  47%|████▋     | 7/15 [00:00<00:00, 12.67 examples/s]Map (num_proc=6):  67%|██████▋   | 10/15 [00:01<00:00, 10.81 examples/s]Map (num_proc=6):  80%|████████  | 12/15 [00:01<00:00, 10.74 examples/s]                                                                        Map (num_proc=5):   0%|          | 0/5 [00:00<?, ? examples/s]Map (num_proc=5):  20%|██        | 1/5 [00:00<00:00,  5.57 examples/s]Map (num_proc=5):  40%|████      | 2/5 [00:00<00:00,  6.62 examples/s]Map (num_proc=5):  80%|████████  | 4/5 [00:00<00:00,  8.03 examples/s]Map (num_proc=5): 100%|██████████| 5/5 [00:00<00:00,  8.15 examples/s]                                                                      /scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/60 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/scratch/work/lunt1/wav2vec2-finetune/finetune.py", line 296, in <module>
    run_train(i, processor, model, train_dataset, val_dataset, training_args)
  File "/scratch/work/lunt1/wav2vec2-finetune/finetune.py", line 236, in run_train
    trainer.train()
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/transformers/trainer.py", line 1662, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/transformers/trainer.py", line 1927, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/wav2vec2-finetune/helper/classes.py", line 88, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/transformers/trainer.py", line 2731, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1684, in forward
    outputs = self.wav2vec2(
              ^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1320, in forward
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 887, in forward
    layer_outputs = layer(
                    ^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 719, in forward
    hidden_states, attn_weights, _ = self.attention(
                                     ^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 596, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/nn/functional.py", line 1843, in softmax
    ret = input.softmax(dim)
          ^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 824.00 MiB (GPU 0; 15.89 GiB total capacity; 13.97 GiB already allocated; 485.88 MiB free; 14.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  0%|          | 0/60 [00:01<?, ?it/s]
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2900) of binary: /scratch/work/lunt1/.conda_envs/w2v2/bin/python
Traceback (most recent call last):
  File "/scratch/work/lunt1/.conda_envs/w2v2/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/work/lunt1/.conda_envs/w2v2/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-06-27_11:55:47
  host      : gpu26.int.triton.aalto.fi
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2900)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: gpu26: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=19622646.0
